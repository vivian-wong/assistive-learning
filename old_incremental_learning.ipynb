{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device0 _CudaDeviceProperties(name='GeForce GTX 1080 Ti', total_memory=11171MB)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import time\n",
    "from sys import platform\n",
    "\n",
    "from models import *\n",
    "from utils.datasets import *\n",
    "from utils.utils import *\n",
    "from torch.utils.data import DataLoader\n",
    "import shutil \n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "\n",
    "device = torch_utils.select_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = 'weights/s100e20.pt' # to be filled in \n",
    "\n",
    "# default, normally don't change \n",
    "cfg = 'cfg/yolov3-spp.cfg'\n",
    "data_cfg = 'data/GDXray.data'\n",
    "output = 'output'\n",
    "img_size = 416\n",
    "conf_thres = 0.1 # 0.001 in original code \n",
    "nms_thres = 0.5 # iou threshold for non-maximum suppression\n",
    "iou_thres = 0.9 # originally 0.5\n",
    "batch_size = 32\n",
    "save_json = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images that are in testing data in data_cfg \n",
    "data_cfg = parse_data_cfg(data_cfg)\n",
    "nc = int(data_cfg['classes'])  # number of classes\n",
    "test_path = data_cfg['valid']  # path to test images\n",
    "names = load_classes(data_cfg['names'])  # class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C0001_0069.png', 'C0055_0014.png', 'C0047_0021.png', 'C0007_0019.png', 'C0019_0024.png']\n"
     ]
    }
   ],
   "source": [
    "# read lines in test file for img names of a few images \n",
    "image_ids = []\n",
    "with open(test_path,\"r\") as f:\n",
    "    image_ids += f.readlines()\n",
    "\n",
    "# Strip all the newlines\n",
    "image_ids = [p.rstrip() for p in image_ids]\n",
    "\n",
    "sampled_image_ids = np.random.choice (image_ids, 5)   \n",
    "print([os.path.basename(p) for p in sampled_image_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vivianPlot(img_path, im0, n=0):\n",
    "    # ground truth \n",
    "    plt.subplot(1,2,1)\n",
    "    image_dir = \"data/GDXray/images\"\n",
    "    image_filename = os.path.basename(img_path)\n",
    "    for roots,_, files in os.walk(image_dir):\n",
    "        for f in files:\n",
    "            if f == image_filename:\n",
    "                impath = os.path.join(roots,f)\n",
    "                im = np.array(Image.open(impath))\n",
    "                lpath = impath.replace(\"images\",\"labels\")\n",
    "                lpath = lpath.replace(\".png\",\".txt\")\n",
    "                break\n",
    "    # Display the image\n",
    "    plt.imshow(im)\n",
    "    \n",
    "    labels = np.loadtxt(lpath)\n",
    "    if labels.ndim <= 1: \n",
    "        labels = np.array([labels])\n",
    "    for l in labels:\n",
    "        w = l[3]*416\n",
    "        h = l[4]*416\n",
    "        botx = l[1]*416-w/2\n",
    "        boty = l[2]*416-h/2\n",
    "        plt.gca().add_patch(Rectangle((botx,boty),w,h,linewidth=1,edgecolor='r',facecolor='none'))\n",
    "    \n",
    "    plt.xlabel(str(len(labels)) + \" defects in ground truth\")\n",
    "    \n",
    "    if n == 0:\n",
    "        plt.show()\n",
    "        return \n",
    "    \n",
    "#     predicted \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(im0)\n",
    "    plt.xlabel(str(n) + \" defects predicted \")\n",
    "    \n",
    "    plt.title(os.path.basename(img_path))\n",
    "    plt.show()\n",
    "\n",
    "def vivianDetect(detections, img_size, image_path):\n",
    "    im0 = cv2.imread(image_path) # BGR\n",
    "    if detections is not None and len(detections) > 0:\n",
    "        # Rescale boxes from 416 to true image size\n",
    "        scale_coords(img_size, detections[:, :4], im0.shape).round()\n",
    "\n",
    "        # Print results to screen\n",
    "#         for c in detections[:, -1].unique(): # got error because 2 defect boxes overlaped\n",
    "        for c in range(0):\n",
    "#             print(detections)\n",
    "#             print(detections[:,-1])\n",
    "#             print(c, detections[:, -1].unique())\n",
    "            n = (detections[:, -1] == c).sum()\n",
    "            print('%g %ss' % (n, classes[int(c)]), end=', ')\n",
    "\n",
    "        # Draw bounding boxes and labels of detections\n",
    "        for *xyxy, conf, cls_conf, cls in detections:\n",
    "            cls = 0 # added to ignore overlapping \n",
    "            color = [0,0,255]\n",
    "\n",
    "            # Add bbox to the image\n",
    "            plot_one_box(xyxy, im0, color=color)\n",
    "#             label = '%s %.2f' % (classes[int(cls)], conf)\n",
    "#             plot_one_box(xyxy, im0, label=label, color=colors[int(cls)])\n",
    "    \n",
    "        n = (detections[:, -1] == 0).sum()\n",
    "#         vivianPlot(image_path, im0, int(n))\n",
    "    else: \n",
    "        print('0 casting defects', end=', ')\n",
    "#         vivianPlot(image_path, im0, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing mAP:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Class    Images   Targets         P         R       mAP        F1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing mAP:  71%|███████▏  | 5/7 [00:02<00:00,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 casting defects, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing mAP: 100%|██████████| 7/7 [00:02<00:00,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       200       715    0.0162    0.0238   0.00205    0.0193\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Y = 0 # for counting # defects in truth \n",
    "y = 0 # for counting # defects predicted  \n",
    "num_correct = 0 # number of predicted defects above iou threshold \n",
    "\n",
    "torch.set_grad_enabled(False)    \n",
    "# Initialize model\n",
    "model = Darknet(cfg, img_size)\n",
    "\n",
    "# Load weights\n",
    "if weights.endswith('.pt'):  # pytorch format\n",
    "    model.load_state_dict(torch.load(weights, map_location=device)['model'])\n",
    "else:  # darknet format\n",
    "    _ = load_darknet_weights(model, weights)\n",
    "\n",
    "model.to(device).eval()\n",
    "\n",
    "# Dataloader\n",
    "dataset = LoadImagesAndLabels(test_path, img_size=img_size)\n",
    "dataloader = DataLoader(dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        num_workers=4,\n",
    "                        pin_memory=False,\n",
    "                        collate_fn=dataset.collate_fn)\n",
    "\n",
    "seen = 0\n",
    "model.eval()\n",
    "coco91class = coco80_to_coco91_class()\n",
    "print(('%20s' + '%10s' * 6) % ('Class', 'Images', 'Targets', 'P', 'R', 'mAP', 'F1'))\n",
    "loss, p, r, f1, mp, mr, map, mf1 = 0., 0., 0., 0., 0., 0., 0., 0.\n",
    "jdict, stats, ap, ap_class = [], [], [], []\n",
    "for batch_i, (imgs, targets, paths, shapes) in enumerate(tqdm(dataloader, desc='Computing mAP')):\n",
    "    targets = targets.to(device)\n",
    "    imgs = imgs.to(device)\n",
    "\n",
    "    # Plot images with bounding boxes\n",
    "    if batch_i == 0 and not os.path.exists('test_batch0.jpg'):\n",
    "        plot_images(imgs=imgs, targets=targets, fname='test_batch0.jpg')\n",
    "\n",
    "    # Run model\n",
    "    inf_out, train_out = model(imgs)  # inference and training outputs\n",
    "\n",
    "    # Build targets\n",
    "    target_list = build_targets(model, targets)\n",
    "\n",
    "    # Compute loss\n",
    "    loss_i, _ = compute_loss(train_out, target_list)\n",
    "    loss += loss_i.item()\n",
    "\n",
    "    # Run NMS\n",
    "    output = non_max_suppression(inf_out, conf_thres=conf_thres, nms_thres=nms_thres)\n",
    "    \n",
    "    # vivianDetect \n",
    "    for i in range(len(output)):\n",
    "        image_path = paths[i]\n",
    "        if image_path in sampled_image_ids:\n",
    "            # plot it \n",
    "            vivianDetect(output[i],img_size, image_path)\n",
    "    \n",
    "    # Statistics per image\n",
    "    for si, pred in enumerate(output):\n",
    "        labels = targets[targets[:, 0] == si, 1:]\n",
    "        nl = len(labels)\n",
    "        Y += nl\n",
    "        tcls = labels[:, 0].tolist() if nl else []  # target class\n",
    "        seen += 1\n",
    "\n",
    "        if pred is None:\n",
    "            if nl:\n",
    "                stats.append(([], torch.Tensor(), torch.Tensor(), tcls))\n",
    "            continue\n",
    "\n",
    "        # Assign all predictions as incorrect\n",
    "        correct = [0] * len(pred)\n",
    "        y += len(pred)\n",
    "        if nl:\n",
    "            detected = []\n",
    "            tbox = xywh2xyxy(labels[:, 1:5]) * img_size  # target boxes\n",
    "\n",
    "            # Search for correct predictions\n",
    "            for i, (*pbox, pconf, pcls_conf, pcls) in enumerate(pred):\n",
    "\n",
    "                # Break if all targets already located in image\n",
    "                if len(detected) == nl:\n",
    "                    break\n",
    "\n",
    "                # Continue if predicted class not among image classes\n",
    "                if pcls.item() not in tcls:\n",
    "                    continue\n",
    "\n",
    "                # Best iou, index between pred and targets\n",
    "                iou, bi = bbox_iou(pbox, tbox).max(0)\n",
    "\n",
    "                # If iou > threshold and class is correct mark as correct\n",
    "                if iou > iou_thres and bi not in detected:\n",
    "                    correct[i] = 1\n",
    "                    num_correct += 1\n",
    "                    detected.append(bi)\n",
    "        # Append statistics (correct, conf, pcls, tcls)\n",
    "        stats.append((correct, pred[:, 4].cpu(), pred[:, 6].cpu(), tcls))\n",
    "\n",
    "# Compute statistics\n",
    "stats_np = [np.concatenate(x, 0) for x in list(zip(*stats))]\n",
    "nt = np.bincount(stats_np[3].astype(np.int64), minlength=nc)  # number of targets per class\n",
    "if len(stats_np):\n",
    "    p, r, ap, f1, ap_class = ap_per_class(*stats_np)\n",
    "    mp, mr, map, mf1 = p.mean(), r.mean(), ap.mean(), f1.mean()\n",
    "\n",
    "# Print results\n",
    "pf = '%20s' + '%10.3g' * 6  # print format\n",
    "print(pf % ('all', seen, nt.sum(), mp, mr, map, mf1), end='\\n\\n')\n",
    "\n",
    "# Print results per class\n",
    "if nc > 1 and len(stats_np):\n",
    "    for i, c in enumerate(ap_class):\n",
    "        print(pf % (names[c], seen, nt[c], p[i], r[i], ap[i], f1[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715 1050 17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f30e1d56438>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # defects in data, # predicted, # predicted correctly\n",
    "print(Y,y,num_correct)\n",
    "torch.set_grad_enabled(True)\n",
    "# plot the results of the same images \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with new sampling Method (source: medAL paper) \n",
    "#### maximize the average distance to all training set examples in a learned feature space (i.e. pixels) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: a list of all trained image paths, a list of all available image paths \n",
    "# output: append full image path of the next image that should be annotated to new_metadata\n",
    "#         a distances_to_trained dictionary storing {img path:np array of distances to all imgs in trained_imgs}\n",
    "def sampleNextImage (distances_to_trained,\n",
    "                     trained_image_metadata= \"./metadata/GDXray/medAL_sampling.txt\", \n",
    "                     all_image_metadata = \"./metadata/GDXray/castings_shuffled_685.txt\", \n",
    "                     new_metadata = \"./metadata/GDXray/medAL_sampling.txt\"):\n",
    "    if trained_image_metadata != new_metadata:\n",
    "        shutil.copy(trained_image_metadata, new_metadata)\n",
    "        \n",
    "    # parse txt files\n",
    "    def read_paths(metadata):\n",
    "        image_paths = []\n",
    "        with open(metadata,\"r\") as f:\n",
    "            image_paths += f.readlines()\n",
    "\n",
    "        # Strip all the newlines\n",
    "        image_paths = [p.rstrip() for p in image_paths]\n",
    "        return image_paths\n",
    "    trained_image_paths = read_paths(trained_image_metadata)\n",
    "    all_image_paths = read_paths(all_image_metadata)\n",
    "    \n",
    "    # make a list of trained images as np arrays\n",
    "    trained_images = []\n",
    "    trained_specimens = defaultdict(int) # {specimen:count of this specimen in trained data}\n",
    "    for p in trained_image_paths:\n",
    "        im = cv2.imread(p).flatten() # shape = (416*416*3,)\n",
    "        trained_images.append(im)\n",
    "        trained_specimens[os.path.basename(os.path.split(p)[0])] += 1\n",
    "    \n",
    "    \n",
    "    # compute distances \n",
    "    untrained_images = []\n",
    "    max_avg_dist = -1\n",
    "    max_avg_dist_unique_specimen = -1\n",
    "    next_image_path = None\n",
    "    for p in all_image_paths: \n",
    "        if p not in trained_image_paths:\n",
    "            # modify dictionary so that every value is subtracted by min . that way we can pick the specimen with least images\n",
    "            min_occurance = min(trained_specimens.values())\n",
    "            for spec in trained_specimens:\n",
    "                trained_specimens[spec] -= min_occurance\n",
    "            \n",
    "            im = cv2.imread(p).flatten()\n",
    "            untrained_images.append(im)\n",
    "            if p not in distances_to_trained:  \n",
    "                distances = np.sum(np.square(im - trained_images), axis = 1) # actually is distance squared, length = # trained img\n",
    "                distances_to_trained[p] = distances\n",
    "            else:\n",
    "                assert(len(distances_to_trained[p]) == len(trained_images)-1)\n",
    "                distance_to_last_sampled = np.sum(np.square(im - trained_images[-1])) # len = 0\n",
    "                distances_to_trained[p]=np.append(distances_to_trained[p],distance_to_last_sampled)\n",
    "                distances = distances_to_trained[p]\n",
    "            \n",
    "            # find next best image\n",
    "            # find the image that has a specimen that has not been chosen yet AND with max avg distance \n",
    "            specimen=os.path.basename(os.path.split(p)[0])\n",
    "            avg_dist = np.mean(distances)\n",
    "            \n",
    "            if avg_dist > max_avg_dist_unique_specimen and (specimen not in trained_specimens or trained_specimens[specimen]==0):\n",
    "                next_image_path = p \n",
    "                max_avg_dist_unique_specimen = avg_dist \n",
    "            if avg_dist > max_avg_dist:\n",
    "                max_avg_dist = avg_dist \n",
    "                if max_avg_dist_unique_specimen == -1:\n",
    "                    next_image_path = p\n",
    "    \n",
    "    # write next_image_path to new_metadata\n",
    "    with open(new_metadata,\"a\") as f: \n",
    "        f.write(\"\\n\" + next_image_path)\n",
    "    \n",
    "    print(\"Wrote \" + next_image_path + \" to \" + new_metadata)\n",
    "    print(\"which now has \" + str(len(trained_images) + 1) + \" images\")\n",
    "\n",
    "    return distances_to_trained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import train\n",
    "import test \n",
    "\n",
    "distances_to_trained = {}\n",
    "training_sizes = [50,100,200,300,400,500,600,685] \n",
    "epochs = 200 # start with 200 epochs at 100 training size \n",
    "\n",
    "for j in range(len(training_sizes)-1):\n",
    "    clear_output()\n",
    "    training_size = training_sizes[j+1]\n",
    "    inc_size = training_sizes[j+1]-training_sizes[j]\n",
    "    \n",
    "    for i in range(inc_size):\n",
    "        if j ==0 and i == 0: \n",
    "            distances_to_trained = sampleNextImage(distances_to_trained, \"./metadata/GDXray/castings_shuffled_50.txt\")\n",
    "        else:\n",
    "            distances_to_trained = sampleNextImage(distances_to_trained)\n",
    "    \n",
    "    train.train(cfg = 'cfg/yolov3-spp.cfg',\n",
    "                data_cfg = 'data/GDXray.data',\n",
    "                epochs=epochs,\n",
    "                resume = True)\n",
    "    shutil.copy('weights/latest.pt','weights/medAL'+str(training_size)+'.pt')\n",
    "    epochs += 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote data/GDXray/images/Castings/C0026/C0026_0014.png to ./metadata/GDXray/vivian_sampling.txt\n",
      "which now has 53 images\n"
     ]
    }
   ],
   "source": [
    "dists=sampleNextImage({},\n",
    "                      \"./metadata/GDXray/vivian_sampling.txt\",\n",
    "                      \"./metadata/GDXray/castings_shuffled_100.txt\",\n",
    "                      \"./metadata/GDXray/vivian_sampling.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data/GDXray/images/Castings/C0010/C0010_0030.png': array([39618483, 33565185, 40044417, 40832778, 39675195, 43289613, 42701889, 30460929, 37239843, 39691671, 42055782, 43307739, 37556904], dtype=uint64),\n",
       " 'data/GDXray/images/Castings/C0001/C0001_0023.png': array([43532907, 46545405, 43404693, 43060260, 45092793, 40723203, 42201585, 40628223, 42528807, 43391025, 20309604, 43166781, 41369772], dtype=uint64)}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39233879.07692308\n",
      "41227312.15384615\n"
     ]
    }
   ],
   "source": [
    "for key,value in dists.items():\n",
    "    print(np.mean(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
